{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerias que vamos a usar\n",
    "import requests\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Crear listas para guardar los datos\n",
    "marcas = []\n",
    "gas95s = []\n",
    "gas98s = []\n",
    "gasoils = []\n",
    "marcas = []\n",
    "direcciones = []\n",
    "poblaciones = []\n",
    "horarios = []\n",
    "\n",
    "# Definimos la funcion que busca y recupera los enlaces de las gasolineras de Barcelona\n",
    "def get_comunity_links(url):\n",
    "    comu_links = []\n",
    "    provi_links = []\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "    except URLError:\n",
    "        print(\"Server down or incorrect domain\")\n",
    "    else:\n",
    "        res = BeautifulSoup(page.content,\"html.parser\")\n",
    "        for link in res.find_all('a'):\n",
    "            if link.has_attr('href'):\n",
    "                if '/c/precio-gasoil-calefaccion-' in link.attrs['href']:\n",
    "                    comu_links.append(link.attrs['href'])\n",
    "                if '/p/precio-gasoil-calefaccion-' in link.attrs['href']:\n",
    "                    provi_links.append(link.attrs['href'])\n",
    "    return comu_links, provi_links\n",
    "\n",
    "def get_provincia_links(comu_links,provi_links):\n",
    "    #provi_links = []\n",
    "    for comu_link in comu_links:\n",
    "        try:\n",
    "            html = (\"https://www.clickgasoil.com\" + comu_link)\n",
    "            page = requests.get(html)\n",
    "        except HTTPError as e:\n",
    "            print(e)\n",
    "        except URLError:\n",
    "            print(\"Server down or incorrect domain\")\n",
    "        else:\n",
    "            #res = BeautifulSoup(open(page, 'r'),\"html.parser\",from_encoding=\"iso-8859-1\")\n",
    "            res = BeautifulSoup(page.content,\"html.parser\", from_encoding=\"latin1\")\n",
    "        for link in res.find_all('a'):\n",
    "            if link.has_attr('href'):\n",
    "                if '/p/precio-gasoil-calefaccion-' in link.attrs['href']:\n",
    "                    provi_links.append(link.attrs['href'])\n",
    "                if '/p/precio-de-gasoil-calefaccion-' in link.attrs['href']:\n",
    "                    provi_links.append(link.attrs['href'])\n",
    "    return provi_links\n",
    "\n",
    "def get_city_links(provi_links):\n",
    "    city_links = []\n",
    "    for provi_link in provi_links:\n",
    "        try:\n",
    "            html = (\"https://www.clickgasoil.com\" + provi_link)\n",
    "            page = requests.get(html)\n",
    "        except HTTPError as e:\n",
    "            print(e)\n",
    "        except URLError:\n",
    "            print(\"Server down or incorrect domain\")\n",
    "        else:\n",
    "            #soup = BeautifulSoup(open(page, 'r'),\"html.parser\",from_encoding=\"iso-8859-1\")\n",
    "            soup = BeautifulSoup(page.content,\"html.parser\", from_encoding=\"latin1\")\n",
    "        for link in soup.find_all('a'):\n",
    "            if link.has_attr('href'):\n",
    "                if '/m/precio-gasoil-calefaccion-' in link.attrs['href']:\n",
    "                    city_links.append(link.attrs['href'])\n",
    "                if '/m/precio-de-gasoil-calefaccion-' in link.attrs['href']:\n",
    "                    city_links.append(link.attrs['href'])\n",
    "    return city_links\n",
    "\n",
    "def get_station_links(city_links):\n",
    "    station_links = []\n",
    "    for city_link in city_links:\n",
    "        try:\n",
    "            html = (\"https://www.clickgasoil.com\" + city_link)\n",
    "            page = requests.get(html)\n",
    "        except HTTPError as e:\n",
    "            print(e)\n",
    "        except URLError:\n",
    "            print(\"Server down or incorrect domain\")\n",
    "        else:\n",
    "            # Estructuramos el contenido de la web obtenido para ser consultado facilmente\n",
    "            #estacion = BeautifulSoup(open(html, 'r'),\"html.parser\",from_encoding=\"iso-8859-1\")\n",
    "            estacion = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "            #Buscamos los links:\n",
    "            for link in estacion.find_all('a'):\n",
    "                if link.has_attr('href'):\n",
    "                    if '/g/' in link.attrs['href']:\n",
    "                        station_links.append(link.attrs['href'])\n",
    "    return station_links\n",
    "\n",
    "# Definimos la funcion que busca y recupera todos los datos referente a cada gasolinera\n",
    "def get_info(station):\n",
    "    try:\n",
    "        html = (\"https://www.clickgasoil.com\" + station)\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36\"}\n",
    "        resp = requests.get(html, headers=headers)\n",
    "        http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "        html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "        encoding = html_encoding or http_encoding\n",
    "        #page = requests.get(html)\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "    except URLError:\n",
    "        print(\"Server down or incorrect domain\")\n",
    "    else:\n",
    "        # Estructuramos el contenido de la web obtenido para ser consultado facilmente\n",
    "        #soup = BeautifulSoup.BeautifulSoup(content.decode('utf-8','ignore'))\n",
    "        #soup = BeautifulSoup(page.content,\"html.parser\", from_encoding=\"latin1\")\n",
    "        soup = BeautifulSoup(resp.content, 'lxml', from_encoding=encoding)\n",
    "        \n",
    "        # Buscamos la información general de cada gasolinera: Marca, Direcion, Poblacion, Horario\n",
    "        info_gasolinera = soup.find(\"div\", {\"class\":\"column small-4 datos_gasolinera\"})\n",
    "        \n",
    "        # Buscamos ahora las etiquetas div que hemos detectado, son las que engloban la información que buscamos\n",
    "        div_lista = info_gasolinera.find_all('div')\n",
    "        \n",
    "        # Recorremos la información y guardamos los datos que nos interesan\n",
    "        for i,dato in enumerate(div_lista):\n",
    "            temp = dato.text.split(\":\")\n",
    "            if (i==0): \n",
    "                marca = temp[1]\n",
    "            if (i==1):\n",
    "                direccion = temp[1]\n",
    "            if (i==2):\n",
    "                poblacion = temp[1]\n",
    "            if (i==3):\n",
    "                horario = temp[1] + temp[2]\n",
    "                \n",
    "        # Buscamos la tabla que contiene los precios de los combustibles a fecha de la consulta\n",
    "        table = soup.find(\"table\", { \"class\" : \"small-12\" })\n",
    "        \n",
    "        # Buscamos todas las filas de esta tabla y las guardamos en una lista\n",
    "        td_list = table.find_all('td')\n",
    "        \n",
    "        # Recorremos la información y guardamos los datos que nos interesan\n",
    "        for i,element in enumerate(td_list):\n",
    "            if(i == 1):\n",
    "                precio_gasoil = td_list[1].text[:-1]\n",
    "                gasoil = precio_gasoil\n",
    "            if (i == 7):\n",
    "                precio_gas95 = td_list[7].text[:-1]\n",
    "                gas95 = precio_gas95\n",
    "            if (i == 9):\n",
    "                precio_gas98 = td_list[9].text[:-1]\n",
    "                gas98 = precio_gas98\n",
    "            \n",
    "        # Devolvemos los datos\n",
    "    return marca,gasoil,gas95,gas98,direccion,poblacion,horario\n",
    "\n",
    "def precios_gasolineras(station_links):\n",
    "    # Para cada estacion, vamos a su pagina donde obtenemos todos sus datos y los precios del combustible del dia\n",
    "    for station in station_links:\n",
    "        marca,gasoil,gas95,gas98,direccion,poblacion,horario = get_info(station)\n",
    "\n",
    "        # Guardamos los datos en listas que nos permitiran luego crear un dataframe que exportaremos.\n",
    "        marcas.append(marca)\n",
    "        gasoils.append(gasoil)\n",
    "        gas95s.append(gas95)\n",
    "        gas98s.append(gas98)\n",
    "        direcciones.append(direccion)\n",
    "        poblaciones.append(poblacion)\n",
    "        horarios.append(horario)\n",
    "\n",
    "    # Creamos un dataframe con todos los datos obtenidos\n",
    "    df = pd.DataFrame(\n",
    "        {'Marca':marcas,\n",
    "         'Gasolina 95':gas95s,\n",
    "         'Gasolina 98':gas98s,\n",
    "         'Gasóleo A':gasoils,\n",
    "         'Direccion':direcciones,\n",
    "         'Poblacion':poblaciones,\n",
    "         'Horario':horarios,\n",
    "         'Fecha': datetime.datetime.now()})\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Parseando la web ClickGasoil que nos ofrece los precios de Gasóleo A, Gasolina 95 y Gasolina 98 de cada gasolinera de Barcelona\n",
    "url = \"https://www.clickgasoil.com/c/precio-gasoil-calefaccion\"\n",
    "comu_links, provi_links = get_comunity_links(url)\n",
    "#print (len(comu_links))\n",
    "#print (len(provi_links))\n",
    "provi_links = get_provincia_links(comu_links,provi_links)\n",
    "#print (len(provi_links)) ## 52 provincias\n",
    "city_links = get_city_links(provi_links)\n",
    "\n",
    "\n",
    "#provincia = 'barcelona'\n",
    "#city_links_query = []\n",
    "\n",
    "    #print (link)\n",
    "    #if 'barcelona' in link:\n",
    "        #city_links_query.append(city_links[i])\n",
    "        \n",
    "#print (len(city_links_query))\n",
    "city_links2 = np.random.choice(city_links,300)\n",
    "#print (city_links2)\n",
    "#print (len(city_links2))\n",
    "#print (city_links[1:10])\n",
    "station_links = get_station_links(city_links2)\n",
    "#query_links = []\n",
    "station_links2 = np.random.choice(station_links,1000)\n",
    "#print (query_links)\n",
    "\n",
    "\n",
    "df = precios_gasolineras(station_links)\n",
    "df.head(10)\n",
    "\n",
    "# Exportamos los datos en un archivos csv\n",
    "df.to_csv(\"gasolineras_precios_pruebas_espanya.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['532525-barcelona-tes2','532525-madrid-tes2','532525-barcelona-tes2']\n",
    "for i,text in enumerate(texts):\n",
    "    if 'mad' in text:\n",
    "        print (texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.589\n"
     ]
    }
   ],
   "source": [
    "text = \"1.589€\"\n",
    "x = text[:-1]\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
